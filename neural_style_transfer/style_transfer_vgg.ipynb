{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AditeyaAItronics/devsoc-bits/blob/main/neural_style_transfer/style_transfer_vgg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6818c7df",
      "metadata": {
        "id": "6818c7df"
      },
      "source": [
        "## ðŸ§  Neural Style Transfer using VGG-19\n",
        "\n",
        "In this task, youâ€™ll implement **Neural Style Transfer (NST)** using a **pre-trained VGG-19** model. This will help you deepen your understanding of convolutional neural networks (CNNs), feature extraction, and how deep learning can be used for image generation.\n",
        "\n",
        "---\n",
        "\n",
        "### âœ… Objectives\n",
        "\n",
        "- Build a Neural Style Transfer pipeline using **VGG-19**.\n",
        "- Use only the **first convolutional layer** from **each of the five convolutional blocks** in VGG-19.\n",
        "    - These layers strike a balance between general texture features and higher-level abstraction.\n",
        "    - Deeper layers become too specialized for object recognition and are less effective for capturing style.\n",
        "\n",
        "---\n",
        "\n",
        "### âš™ï¸ System Requirements\n",
        "\n",
        "VGG-19 is a **computationally heavy** model. If your system struggles to run it:\n",
        "\n",
        "- Use **Google Collab** to access free GPU resources.\n",
        "- This will allow faster computation and smoother experimentation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "487eac4b",
      "metadata": {
        "id": "487eac4b"
      },
      "source": [
        "### ðŸ“„ Reference Paper\n",
        "\n",
        "Weâ€™ve linked the paper on **CNN-based image style transformation** below.\n",
        "\n",
        "It uses VGG-19 and provides insight into the theory behind NST.\n",
        "\n",
        "> ðŸ”— https://drive.google.com/file/d/1Dbxaazv-L2SbC3gY4cPlqOQmM2iGmwyB/view\n",
        ">\n",
        "\n",
        "Please read it carefullyâ€”it will help you understand whatâ€™s going on inside the model and how different layers contribute to the stylization process."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43cde8b7",
      "metadata": {
        "id": "43cde8b7"
      },
      "source": [
        "## Implementation thought process\n",
        "\n",
        "- Load Pre-trained VGG-19\n",
        " Use a pre-trained VGG-19 model from PyTorch (torchvision.models.vgg19(pretrained=True)) or TensorFlow (tf.keras.applications.VGG19).\n",
        " Set the model to evaluation mode and freeze its weights.\n",
        "\n",
        "- Select the First Conv Layer from Each Block\n",
        " In VGG-19, the first convolutional layers of the five blocks are typically named: conv1_1, conv2_1, conv3_1, conv4_1, conv5_1. Extract outputs from these layers for style and content representations.\n",
        "\n",
        "- Build the NST Pipeline\n",
        "\n",
        "  - Preprocess Images: Resize, normalize, and convert images to tensors.\n",
        "  - Extract Features: Pass the content and style images through the model, capturing activations from the selected layers.\n",
        "  - Compute Losses:\n",
        "     - Content Loss: Usually from conv4_1.\n",
        "     - Style Loss: Use Gram matrices from all five selected layers.\n",
        "- Optimization: Start with the content image (or white noise) and iteratively update it to minimize the combined loss."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40cf1e81",
      "metadata": {
        "id": "40cf1e81"
      },
      "source": [
        "## NST Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a2574b5a",
      "metadata": {
        "id": "a2574b5a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "00fde93e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00fde93e",
        "outputId": "b51d1fe4-3aec-4d95-920f-2a3fd6b03573"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "vgg = models.vgg19(pretrained=True).features.eval()\n",
        "for param in vgg.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "47a92151",
      "metadata": {
        "id": "47a92151"
      },
      "outputs": [],
      "source": [
        "## layer selection\n",
        "# The indices of the layers we want to use for feature extraction\n",
        "selected_layers = {'0': 'conv1_1', '5': 'conv2_1', '10': 'conv3_1', '19': 'conv4_1', '28': 'conv5_1'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "5680b44f",
      "metadata": {
        "id": "5680b44f"
      },
      "outputs": [],
      "source": [
        "## Preprocessing function\n",
        "def image_loader(image_path, imsize=512):\n",
        "    loader = transforms.Compose([\n",
        "        transforms.Resize((imsize, imsize)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = loader(image).unsqueeze(0)\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "4445ada7",
      "metadata": {
        "id": "4445ada7"
      },
      "outputs": [],
      "source": [
        "## Feature extraction function\n",
        "def get_features(image, model, layers):\n",
        "    features = {}\n",
        "    x = image\n",
        "    for name, layer in model._modules.items():\n",
        "        x = layer(x)\n",
        "        if name in layers:\n",
        "            features[layers[name]] = x\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "34a94a47",
      "metadata": {
        "id": "34a94a47"
      },
      "outputs": [],
      "source": [
        "## style representation function\n",
        "def gram_matrix(tensor):\n",
        "    b, c, h, w = tensor.size()\n",
        "    features = tensor.view(b * c, h * w)\n",
        "    G = torch.mm(features, features.t())\n",
        "    return G.div(b * c * h * w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "abd98e35",
      "metadata": {
        "id": "abd98e35"
      },
      "outputs": [],
      "source": [
        "## loss function\n",
        "content_weight = 1e4\n",
        "style_weight = 1e2\n",
        "\n",
        "def compute_content_loss(gen_feat, content_feat):\n",
        "    return torch.mean((gen_feat - content_feat) ** 2)\n",
        "\n",
        "def compute_style_loss(gen_feats, style_feats):\n",
        "    style_loss = 0\n",
        "    for layer in style_feats:\n",
        "        G = gram_matrix(gen_feats[layer])\n",
        "        A = gram_matrix(style_feats[layer])\n",
        "        style_loss += torch.mean((G - A) ** 2)\n",
        "    return style_loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/AditeyaAItronics/devsoc-bits"
      ],
      "metadata": {
        "id": "rfRo5OfSbgCT",
        "outputId": "0c218662-a0ad-4a4e-bcdb-a23485d85c48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "rfRo5OfSbgCT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'devsoc-bits' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90ae8312",
      "metadata": {
        "id": "90ae8312"
      },
      "outputs": [],
      "source": [
        "content_img_path = '/content/devsoc-bits/neural_style_transfer/images/test_images/alien.jpg'\n",
        "style_img_path = '/content/devsoc-bits/neural_style_transfer/images/style/mosaic.jpg'\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load images\n",
        "content_img = image_loader(content_img_path).to(device)\n",
        "style_img = image_loader(style_img_path).to(device)\n",
        "\n",
        "# Extract features\n",
        "content_features = get_features(content_img, vgg, selected_layers)\n",
        "style_features = get_features(style_img, vgg, selected_layers)\n",
        "\n",
        "# Initialize generated image (copy of content image, or use torch.randn_like(content_img) for white noise)\n",
        "generated_img = content_img.clone().requires_grad_(True).to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.LBFGS([generated_img])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0178210d",
      "metadata": {
        "id": "0178210d"
      },
      "outputs": [],
      "source": [
        "num_steps = 300\n",
        "\n",
        "for step in range(num_steps):\n",
        "    def closure():\n",
        "        optimizer.zero_grad()\n",
        "        gen_features = get_features(generated_img, vgg, selected_layers)\n",
        "        content_loss = compute_content_loss(gen_features['conv4_1'], content_features['conv4_1'])\n",
        "        style_loss = compute_style_loss(gen_features, style_features)\n",
        "        total_loss = content_weight * content_loss + style_weight * style_loss\n",
        "        total_loss.backward()\n",
        "        return total_loss\n",
        "\n",
        "    optimizer.step(closure)\n",
        "    if step % 50 == 0:\n",
        "        print(f\"Step {step}:\")\n",
        "        with torch.no_grad():\n",
        "            gen_features = get_features(generated_img, vgg, selected_layers)\n",
        "            content_loss = compute_content_loss(gen_features['conv4_1'], content_features['conv4_1'])\n",
        "            style_loss = compute_style_loss(gen_features, style_features)\n",
        "            total_loss = content_weight * content_loss + style_weight * style_loss\n",
        "            print(f\"Content Loss: {content_loss.item():.4f}, Style Loss: {style_loss.item():.4f}, Total Loss: {total_loss.item():.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "daa8736c",
      "metadata": {
        "id": "daa8736c"
      },
      "outputs": [],
      "source": [
        "# To display or save the generated image, de-normalize and convert to PIL\n",
        "def im_convert(tensor):\n",
        "    image = tensor.clone().detach().cpu().squeeze(0)\n",
        "    image = image * torch.tensor([0.229, 0.224, 0.225]).view(3,1,1) + torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
        "    image = image.clamp(0, 1)\n",
        "    return transforms.ToPILImage()(image)\n",
        "\n",
        "output_image = im_convert(generated_img)\n",
        "output_image.save('output.png')\n",
        "output_image.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}