{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AditeyaAItronics/devsoc-bits/blob/main/neural_style_transfer/style_transfer_vgg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6818c7df",
      "metadata": {
        "id": "6818c7df"
      },
      "source": [
        "## 🧠 Neural Style Transfer using VGG-19\n",
        "\n",
        "In this task, you’ll implement **Neural Style Transfer (NST)** using a **pre-trained VGG-19** model. This will help you deepen your understanding of convolutional neural networks (CNNs), feature extraction, and how deep learning can be used for image generation.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Objectives\n",
        "\n",
        "- Build a Neural Style Transfer pipeline using **VGG-19**.\n",
        "- Use only the **first convolutional layer** from **each of the five convolutional blocks** in VGG-19.\n",
        "    - These layers strike a balance between general texture features and higher-level abstraction.\n",
        "    - Deeper layers become too specialized for object recognition and are less effective for capturing style.\n",
        "\n",
        "---\n",
        "\n",
        "### ⚙️ System Requirements\n",
        "\n",
        "VGG-19 is a **computationally heavy** model. If your system struggles to run it:\n",
        "\n",
        "- Use **Google Collab** to access free GPU resources.\n",
        "- This will allow faster computation and smoother experimentation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "487eac4b",
      "metadata": {
        "id": "487eac4b"
      },
      "source": [
        "### 📄 Reference Paper\n",
        "\n",
        "We’ve linked the paper on **CNN-based image style transformation** below.\n",
        "\n",
        "It uses VGG-19 and provides insight into the theory behind NST.\n",
        "\n",
        "> 🔗 https://drive.google.com/file/d/1Dbxaazv-L2SbC3gY4cPlqOQmM2iGmwyB/view\n",
        ">\n",
        "\n",
        "Please read it carefully—it will help you understand what’s going on inside the model and how different layers contribute to the stylization process."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43cde8b7",
      "metadata": {
        "id": "43cde8b7"
      },
      "source": [
        "## Implementation thought process\n",
        "\n",
        "- Load Pre-trained VGG-19\n",
        " Use a pre-trained VGG-19 model from PyTorch (torchvision.models.vgg19(pretrained=True)) or TensorFlow (tf.keras.applications.VGG19).\n",
        " Set the model to evaluation mode and freeze its weights.\n",
        "\n",
        "- Select the First Conv Layer from Each Block\n",
        " In VGG-19, the first convolutional layers of the five blocks are typically named: conv1_1, conv2_1, conv3_1, conv4_1, conv5_1. Extract outputs from these layers for style and content representations.\n",
        "\n",
        "- Build the NST Pipeline\n",
        "\n",
        "  - Preprocess Images: Resize, normalize, and convert images to tensors.\n",
        "  - Extract Features: Pass the content and style images through the model, capturing activations from the selected layers.\n",
        "  - Compute Losses:\n",
        "     - Content Loss: Usually from conv4_1.\n",
        "     - Style Loss: Use Gram matrices from all five selected layers.\n",
        "- Optimization: Start with the content image (or white noise) and iteratively update it to minimize the combined loss."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40cf1e81",
      "metadata": {
        "id": "40cf1e81"
      },
      "source": [
        "## NST Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a2574b5a",
      "metadata": {
        "id": "a2574b5a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "00fde93e",
      "metadata": {
        "id": "00fde93e"
      },
      "outputs": [],
      "source": [
        "vgg = models.vgg19(pretrained=True).features.eval()\n",
        "\n",
        "for param in vgg.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vgg = vgg.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "47a92151",
      "metadata": {
        "id": "47a92151"
      },
      "outputs": [],
      "source": [
        "## layer selection\n",
        "# The indices of the layers we want to use for feature extraction\n",
        "selected_layers = {'0': 'conv1_1', '5': 'conv2_1', '10': 'conv3_1', '19': 'conv4_1', '28': 'conv5_1'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "5680b44f",
      "metadata": {
        "id": "5680b44f"
      },
      "outputs": [],
      "source": [
        "## Preprocessing function\n",
        "def image_loader(image_path, imsize=512):\n",
        "    loader = transforms.Compose([\n",
        "        transforms.Resize((imsize, imsize)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = loader(image).unsqueeze(0)\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "4445ada7",
      "metadata": {
        "id": "4445ada7"
      },
      "outputs": [],
      "source": [
        "## Feature extraction function\n",
        "def get_features(image, model, layers):\n",
        "    features = {}\n",
        "    x = image\n",
        "    for name, layer in model._modules.items():\n",
        "        x = layer(x)\n",
        "        if name in layers:\n",
        "            features[layers[name]] = x\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "34a94a47",
      "metadata": {
        "id": "34a94a47"
      },
      "outputs": [],
      "source": [
        "## style representation function\n",
        "def gram_matrix(tensor):\n",
        "    b, c, h, w = tensor.size()\n",
        "    features = tensor.view(b * c, h * w)\n",
        "    G = torch.mm(features, features.t())\n",
        "    return G.div(b * c * h * w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "abd98e35",
      "metadata": {
        "id": "abd98e35"
      },
      "outputs": [],
      "source": [
        "## loss function\n",
        "content_weight = 1e4\n",
        "style_weight = 1e2\n",
        "\n",
        "def compute_content_loss(gen_feat, content_feat):\n",
        "    return torch.mean((gen_feat - content_feat) ** 2)\n",
        "\n",
        "def compute_style_loss(gen_feats, style_feats):\n",
        "    style_loss = 0\n",
        "    for layer in style_feats:\n",
        "        G = gram_matrix(gen_feats[layer])\n",
        "        A = gram_matrix(style_feats[layer])\n",
        "        style_loss += torch.mean((G - A) ** 2)\n",
        "    return style_loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/AditeyaAItronics/devsoc-bits"
      ],
      "metadata": {
        "id": "rfRo5OfSbgCT",
        "outputId": "0c218662-a0ad-4a4e-bcdb-a23485d85c48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "rfRo5OfSbgCT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'devsoc-bits' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "90ae8312",
      "metadata": {
        "id": "90ae8312"
      },
      "outputs": [],
      "source": [
        "content_img_path = '/content/devsoc-bits/neural_style_transfer/images/test_images/alien.jpg'\n",
        "style_img_path = '/content/devsoc-bits/neural_style_transfer/images/style/picasso.jpg'\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load images\n",
        "content_img = image_loader(content_img_path).to(device)\n",
        "style_img = image_loader(style_img_path).to(device)\n",
        "\n",
        "# Extract features\n",
        "content_features = get_features(content_img, vgg, selected_layers)\n",
        "style_features = get_features(style_img, vgg, selected_layers)\n",
        "\n",
        "# generated_img = content_img.clone().requires_grad_(True).to(device)\n",
        "generated_img = content_img.clone().detach().to(device)\n",
        "generated_img.requires_grad_(True)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.LBFGS([generated_img])\n",
        "# optimizer = torch.optim.Adam([generated_img], lr=0.01)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "0178210d",
      "metadata": {
        "id": "0178210d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b44ffff3-3f74-4c9c-e6ed-a1939adfb5bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: Content Loss: 0.0061, Style Loss: 0.0003, Total Loss: 60.8689\n",
            "Step 20: Content Loss: 0.0391, Style Loss: 0.0003, Total Loss: 390.8276\n",
            "Step 40: Content Loss: 0.0143, Style Loss: 0.0003, Total Loss: 143.3551\n",
            "Step 60: Content Loss: 0.0081, Style Loss: 0.0003, Total Loss: 81.2332\n",
            "Step 80: Content Loss: 0.0058, Style Loss: 0.0003, Total Loss: 58.2111\n",
            "Step 100: Content Loss: 0.0046, Style Loss: 0.0003, Total Loss: 46.4638\n",
            "Step 120: Content Loss: 0.0039, Style Loss: 0.0003, Total Loss: 39.3022\n",
            "Step 140: Content Loss: 0.0035, Style Loss: 0.0003, Total Loss: 34.6906\n",
            "Step 160: Content Loss: 0.0031, Style Loss: 0.0003, Total Loss: 30.6571\n",
            "Step 180: Content Loss: 0.0028, Style Loss: 0.0003, Total Loss: 28.3569\n",
            "Step 200: Content Loss: 0.0027, Style Loss: 0.0003, Total Loss: 26.6253\n",
            "Step 220: Content Loss: 0.0025, Style Loss: 0.0003, Total Loss: 25.3948\n",
            "Step 240: Content Loss: 0.0025, Style Loss: 0.0003, Total Loss: 24.5711\n",
            "Step 260: Content Loss: 0.0023, Style Loss: 0.0003, Total Loss: 23.0571\n",
            "Step 280: Content Loss: 0.0040, Style Loss: 0.0003, Total Loss: 39.5718\n",
            "Step 300: Content Loss: 0.0036, Style Loss: 0.0003, Total Loss: 35.7947\n",
            "Step 320: Content Loss: 0.0033, Style Loss: 0.0003, Total Loss: 33.4231\n",
            "Step 340: Content Loss: 0.0024, Style Loss: 0.0003, Total Loss: 24.3727\n",
            "Step 360: Content Loss: 0.0024, Style Loss: 0.0003, Total Loss: 24.4597\n",
            "Step 380: Content Loss: 0.0043, Style Loss: 0.0003, Total Loss: 43.2218\n",
            "Step 400: Content Loss: 0.0028, Style Loss: 0.0003, Total Loss: 28.1307\n",
            "Step 420: Content Loss: 0.0026, Style Loss: 0.0003, Total Loss: 25.9291\n",
            "Step 440: Content Loss: 0.0023, Style Loss: 0.0003, Total Loss: 23.5289\n",
            "Step 460: Content Loss: 0.0023, Style Loss: 0.0003, Total Loss: 22.8610\n",
            "Step 480: Content Loss: 0.0024, Style Loss: 0.0003, Total Loss: 23.8077\n",
            "Step 500: Content Loss: 0.0026, Style Loss: 0.0003, Total Loss: 25.7481\n",
            "Step 520: Content Loss: 0.0031, Style Loss: 0.0003, Total Loss: 30.5779\n",
            "Step 540: Content Loss: 0.0029, Style Loss: 0.0003, Total Loss: 28.9631\n",
            "Step 560: Content Loss: 0.0033, Style Loss: 0.0003, Total Loss: 33.4288\n",
            "Step 580: Content Loss: 0.0028, Style Loss: 0.0003, Total Loss: 27.8305\n",
            "Step 600: Content Loss: 0.0024, Style Loss: 0.0003, Total Loss: 23.9013\n",
            "Step 620: Content Loss: 0.0040, Style Loss: 0.0003, Total Loss: 39.9822\n",
            "Step 640: Content Loss: 0.0027, Style Loss: 0.0003, Total Loss: 27.3524\n",
            "Step 660: Content Loss: 0.0031, Style Loss: 0.0003, Total Loss: 30.7999\n",
            "Step 680: Content Loss: 0.0051, Style Loss: 0.0003, Total Loss: 51.1453\n",
            "Step 700: Content Loss: 0.0096, Style Loss: 0.0003, Total Loss: 95.5358\n",
            "Step 720: Content Loss: 0.0060, Style Loss: 0.0003, Total Loss: 59.5448\n",
            "Step 740: Content Loss: 0.0056, Style Loss: 0.0003, Total Loss: 55.8066\n",
            "Step 760: Content Loss: 0.0045, Style Loss: 0.0003, Total Loss: 44.7945\n",
            "Step 780: Content Loss: 0.0066, Style Loss: 0.0003, Total Loss: 66.4369\n",
            "Step 800: Content Loss: 0.0052, Style Loss: 0.0003, Total Loss: 51.5985\n",
            "Step 820: Content Loss: 0.0040, Style Loss: 0.0003, Total Loss: 39.8145\n",
            "Step 840: Content Loss: 0.0032, Style Loss: 0.0003, Total Loss: 31.6766\n",
            "Step 860: Content Loss: 0.0028, Style Loss: 0.0003, Total Loss: 28.0535\n",
            "Step 880: Content Loss: 0.0034, Style Loss: 0.0003, Total Loss: 33.5478\n",
            "Step 900: Content Loss: 0.0044, Style Loss: 0.0003, Total Loss: 43.9714\n",
            "Step 920: Content Loss: 0.0053, Style Loss: 0.0003, Total Loss: 53.0112\n",
            "Step 940: Content Loss: 0.0068, Style Loss: 0.0003, Total Loss: 68.3188\n",
            "Step 960: Content Loss: 0.0050, Style Loss: 0.0003, Total Loss: 50.0033\n",
            "Step 980: Content Loss: 0.0037, Style Loss: 0.0003, Total Loss: 37.0163\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.Adam([generated_img], lr=0.015)\n",
        "num_steps = 1000\n",
        "\n",
        "for step in range(num_steps):\n",
        "    optimizer.zero_grad()\n",
        "    gen_features = get_features(generated_img, vgg, selected_layers)\n",
        "    content_loss = compute_content_loss(gen_features['conv4_1'], content_features['conv4_1'])\n",
        "    style_loss = compute_style_loss(gen_features, style_features)\n",
        "    total_loss = content_weight * content_loss + style_weight * style_loss\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "    if step % 20 == 0:\n",
        "        print(f\"Step {step}: Content Loss: {content_loss.item():.4f}, Style Loss: {style_loss.item():.4f}, Total Loss: {total_loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "daa8736c",
      "metadata": {
        "id": "daa8736c"
      },
      "outputs": [],
      "source": [
        "def im_convert(tensor):\n",
        "    image = tensor.clone().detach().cpu().squeeze(0)\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n",
        "    image = image * std + mean\n",
        "    image = image.clamp(0, 1)\n",
        "    return transforms.ToPILImage()(image)\n",
        "\n",
        "output_image = im_convert(generated_img)\n",
        "output_image.save('output.png')\n",
        "output_image.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}